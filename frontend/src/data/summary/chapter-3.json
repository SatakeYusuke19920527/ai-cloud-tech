{
  "slug": "chapter-3",
  "title": "第3章 機械学習の具体的手法",
  "description": "教師あり/なし学習や代表アルゴリズムの要点",
  "keywords": [
    {
      "term": "アンサンブル学習",
      "meaning": "複数のモデルを組み合わせて精度を向上させる手法。"
    },
    {
      "term": "カーネル",
      "meaning": "非線形データを高次元に写像する関数。"
    },
    {
      "term": "カーネルトリック",
      "meaning": "高次元の計算を直接行わず内積だけで処理する仕組み。"
    },
    {
      "term": "回帰問題",
      "meaning": "数値を予測する機械学習タスク。"
    },
    {
      "term": "決定木",
      "meaning": "特徴量による条件分岐で予測するモデル。"
    },
    {
      "term": "勾配ブースティング",
      "meaning": "弱学習器を逐次追加し誤差を改善する手法。"
    },
    {
      "term": "サポートベクターマシン (SVM)",
      "meaning": "マージンを最大化する分類・回帰手法。"
    },
    {
      "term": "線形回帰",
      "meaning": "変数の線形関係を用いて値を予測する手法。"
    },
    {
      "term": "自己回帰モデル (ARモデル)",
      "meaning": "過去の値から未来の値を予測する時系列モデル。"
    },
    {
      "term": "単回帰分析",
      "meaning": "1つの説明変数で目的変数を予測する回帰分析。"
    },
    {
      "term": "重回帰分析",
      "meaning": "複数の説明変数で予測する回帰分析。"
    },
    {
      "term": "多クラス分類",
      "meaning": "3種類以上のクラスを分類するタスク。"
    },
    {
      "term": "バギング",
      "meaning": "データを複数サンプリングして並列学習する手法。"
    },
    {
      "term": "ブースティング",
      "meaning": "誤分類例を重視し改善を重ねる逐次学習手法。"
    },
    {
      "term": "ブートストラップサンプリング",
      "meaning": "元データから復元抽出でサンプルを作る方法。"
    },
    {
      "term": "分類問題",
      "meaning": "カテゴリを予測する機械学習タスク。"
    },
    {
      "term": "ベクトル自己回帰モデル (VARモデル)",
      "meaning": "複数時系列間の相互依存を扱うモデル。"
    },
    {
      "term": "マージン最大化",
      "meaning": "クラス境界とデータの距離を最大にする考え方（SVMの基礎）。"
    },
    {
      "term": "ランダムフォレスト",
      "meaning": "複数の決定木を平均化するアンサンブル手法。"
    },
    {
      "term": "ロジスティック回帰",
      "meaning": "確率を出力し二値分類を行うモデル。"
    },
    {
      "term": "k-means法",
      "meaning": "クラスタ中心を更新しながら分割するクラスタリング手法。"
    },
    {
      "term": "t-SNE",
      "meaning": "高次元データを2〜3次元に可視化する手法。"
    },
    {
      "term": "ウォード法",
      "meaning": "クラスタ間の分散の増加を最小化する階層クラスタリング。"
    },
    {
      "term": "協調フィルタリング",
      "meaning": "類似ユーザやアイテムに基づく推薦手法。"
    },
    {
      "term": "クラスタリング",
      "meaning": "データを似た特徴でグループ化する手法。"
    },
    {
      "term": "コールドスタート問題",
      "meaning": "新規ユーザやアイテムの推薦が難しい問題。"
    },
    {
      "term": "コンテンツベースフィルタリング",
      "meaning": "アイテムの特徴に基づいて推薦する手法。"
    },
    {
      "term": "次元削減",
      "meaning": "特徴量をより少ない次元に圧縮する処理。"
    },
    {
      "term": "主成分分析 (PCA)",
      "meaning": "分散が最大となる軸にデータを写す次元削減手法。"
    },
    {
      "term": "潜在的ディリクレ配分法 (LDA)",
      "meaning": "文書を潜在トピックに分解するトピックモデル。"
    },
    {
      "term": "多次元尺度構成法 (MDS)",
      "meaning": "データ間距離を保ちながら低次元に配置する手法。"
    },
    {
      "term": "デンドログラム (樹形図)",
      "meaning": "階層クラスタリング結果を木構造で示す図。"
    },
    {
      "term": "特異値分解 (SVD)",
      "meaning": "行列を特異ベクトルへ分解する手法（推薦や次元削減に使用）。"
    },
    {
      "term": "トピックモデル",
      "meaning": "文書から潜在的な話題を推定するモデル群。"
    },
    {
      "term": "Actor-Critic",
      "meaning": "価値関数と方策を同時に学習する強化学習手法。"
    },
    {
      "term": "ε-greedy方策",
      "meaning": "一定確率でランダム行動する探索方策。"
    },
    {
      "term": "REINFORCE",
      "meaning": "方策勾配に基づく強化学習アルゴリズム。"
    },
    {
      "term": "Q学習",
      "meaning": "行動価値をテーブル更新で最適化する強化学習手法。"
    },
    {
      "term": "UCB方策",
      "meaning": "未探索の価値を楽観的に見積もるバンディット方策。"
    },
    {
      "term": "行動価値関数",
      "meaning": "状態と行動の価値を表す関数(Q値)。"
    },
    {
      "term": "状態価値関数",
      "meaning": "状態そのものの価値を示す関数(V値)。"
    },
    {
      "term": "バンディットアルゴリズム",
      "meaning": "試行と報酬のトレードオフを扱う最適化手法。"
    },
    {
      "term": "方策勾配法",
      "meaning": "方策（行動確率）を微分して改善する手法。"
    },
    {
      "term": "マルコフ決定過程",
      "meaning": "状態・行動・遷移で構成される強化学習の数学的枠組み。"
    },
    {
      "term": "割引率",
      "meaning": "将来報酬の現在価値を調整するパラメータ。"
    },
    {
      "term": "SARSA",
      "meaning": "状態・行動・報酬・次状態・次行動で更新する強化学習手法。"
    },
    {
      "term": "k-分割交差検証",
      "meaning": "データをk分割し交互に検証する精度評価方法。"
    },
    {
      "term": "平均二乗誤差 (MSE)",
      "meaning": "誤差の二乗平均を取った回帰評価指標。"
    },
    {
      "term": "二乗平均平方根誤差 (RMSE)",
      "meaning": "MSEの平方根。誤差を元の単位で表す。"
    },
    {
      "term": "平均絶対値誤差 (MAE)",
      "meaning": "誤差の絶対値平均を示す評価指標。"
    },
    {
      "term": "ROC曲線・AUC",
      "meaning": "分類性能を閾値に依存せず評価する指標。"
    },
    {
      "term": "赤池情報量規準 (AIC)",
      "meaning": "モデルの複雑さと当てはまりのバランスを測る指標。"
    },
    {
      "term": "オッカムの剃刀",
      "meaning": "最も単純なモデルを選ぶべきという原則。"
    },
    {
      "term": "過学習",
      "meaning": "訓練データに適合しすぎて汎化できない状態。"
    },
    {
      "term": "交差検証",
      "meaning": "データ分割により汎化性能を推定する方法。"
    },
    {
      "term": "偽陽性・偽陰性",
      "meaning": "誤った陽性判定・誤った陰性判定。"
    },
    {
      "term": "真陽性・真陰性",
      "meaning": "正しく陽性・陰性を判定した結果。"
    },
    {
      "term": "混同行列",
      "meaning": "分類結果を4区分で示す表。"
    },
    {
      "term": "正解率・適合率・再現率・F値",
      "meaning": "分類性能を多角的に評価する主要指標。"
    },
    {
      "term": "汎化性能",
      "meaning": "未知データで性能を発揮する能力。"
    },
    {
      "term": "ベイズ情報量規準 (BIC)",
      "meaning": "AICにペナルティを加えたモデル選択指標。"
    },
    {
      "term": "ホールドアウト検証",
      "meaning": "訓練・テストに分割して評価する方法。"
    }
  ]
}