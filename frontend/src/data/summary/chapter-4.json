{
  "slug": "chapter-4",
  "title": "第4章 ディープラーニングの概要",
  "description": "ニューラルネットワークの基本構造と学習プロセス",
  "keywords": [
    {
      "term": "CPU",
      "meaning": "汎用計算を行う中央処理装置。"
    },
    {
      "term": "GPU",
      "meaning": "大量の並列演算が得意な処理装置（深層学習向き）。"
    },
    {
      "term": "TPU",
      "meaning": "Googleが開発した深層学習専用プロセッサ。"
    },
    {
      "term": "隠れ層・入力層・出力層",
      "meaning": "ニューラルネットの基本構造を構成する3つの層。"
    },
    {
      "term": "多層パーセプトロン",
      "meaning": "複数の中間層を持つニューラルネットモデル。"
    },
    {
      "term": "単純パーセプトロン",
      "meaning": "1層のみの基本的なパーセプトロンモデル。"
    },
    {
      "term": "Leaky ReLU関数",
      "meaning": "負の領域に小さな傾きを持つ活性化関数。"
    },
    {
      "term": "ReLU関数",
      "meaning": "0より大きければそのまま返す活性化関数。"
    },
    {
      "term": "tanh関数",
      "meaning": "出力が-1〜1のS字型活性化関数。"
    },
    {
      "term": "シグモイド関数",
      "meaning": "0〜1で出力するS字型活性化関数。"
    },
    {
      "term": "ソフトマックス関数",
      "meaning": "出力を確率分布に変換する関数。"
    },
    {
      "term": "勾配消失問題",
      "meaning": "深い層で勾配が小さくなり学習が進まなくなる問題。"
    },
    {
      "term": "Contrastive Loss",
      "meaning": "類似・非類似のペア学習に使う損失関数。"
    },
    {
      "term": "Triplet Loss",
      "meaning": "アンカー・ポジティブ・ネガティブの距離差で学習する損失関数。"
    },
    {
      "term": "KL情報量",
      "meaning": "2つの確率分布の差を測る指標。"
    },
    {
      "term": "交差エントロピー",
      "meaning": "分類タスクで最もよく用いられる損失関数。"
    },
    {
      "term": "平均二乗誤差関数",
      "meaning": "回帰問題で使われる誤差の二乗平均。"
    },
    {
      "term": "L0正則化",
      "meaning": "非ゼロパラメータ数を抑える正則化。"
    },
    {
      "term": "L1正則化",
      "meaning": "重みの絶対値をペナルティとする正則化（疎な解を得る）。"
    },
    {
      "term": "L2正則化",
      "meaning": "重みの二乗をペナルティとする正則化（安定化）。"
    },
    {
      "term": "正則化",
      "meaning": "過学習を防ぐためモデルの複雑さに制約をかける手法。"
    },
    {
      "term": "ドロップアウト",
      "meaning": "学習中にノードを無効化し過学習を防ぐ手法。"
    },
    {
      "term": "ラッソ回帰",
      "meaning": "L1正則化を用いる線形回帰。"
    },
    {
      "term": "リッジ回帰",
      "meaning": "L2正則化を用いる線形回帰。"
    },
    {
      "term": "勾配爆発問題",
      "meaning": "勾配が過度に大きくなり学習が不安定になる問題。"
    },
    {
      "term": "信用割当問題",
      "meaning": "どの行動が結果に寄与したか判断しづらい問題。"
    },
    {
      "term": "連鎖律",
      "meaning": "複合関数の微分を伝搬する微分法則（バックプロパゲーションの基礎）。"
    },
    {
      "term": "AdaBound",
      "meaning": "Adamに上下限を設け安定させた最適化手法。"
    },
    {
      "term": "AdaDelta",
      "meaning": "学習率を自動調整する最適化手法。"
    },
    {
      "term": "AdaGrad",
      "meaning": "勾配の累積で学習率を調整する最適化手法。"
    },
    {
      "term": "Adam",
      "meaning": "モーメンタムとAdaGradを組み合わせた最適化手法。"
    },
    {
      "term": "AMSBound",
      "meaning": "Adamの学習率を安定化した改良版。"
    },
    {
      "term": "RMSprop",
      "meaning": "勾配の移動平均を利用する最適化手法。"
    },
    {
      "term": "鞍点",
      "meaning": "勾配が0だが最適解でない位置。"
    },
    {
      "term": "イテレーション",
      "meaning": "パラメータ更新の1回の繰り返し。"
    },
    {
      "term": "エポック",
      "meaning": "データ全体を1周学習する単位。"
    },
    {
      "term": "オンライン学習",
      "meaning": "逐次データを使いながら学習する方法。"
    },
    {
      "term": "学習率",
      "meaning": "勾配による更新量を決めるパラメータ。"
    },
    {
      "term": "確率的勾配降下法 (SGD)",
      "meaning": "データを1つずつ使って更新する勾配降下法。"
    },
    {
      "term": "グリッドサーチ",
      "meaning": "候補値を全探索して最適なハイパーパラメータを見つける方法。"
    },
    {
      "term": "勾配降下法",
      "meaning": "勾配方向にパラメータを更新する最適化手法。"
    },
    {
      "term": "局所最適解",
      "meaning": "近傍では最適だが全体では最適でない解。"
    },
    {
      "term": "早期終了",
      "meaning": "性能悪化前に学習を打ち切り過学習を防ぐ方法。"
    },
    {
      "term": "大域最適解",
      "meaning": "全体で最も良い解。"
    },
    {
      "term": "二重降下現象",
      "meaning": "モデルの複雑さが増えても誤差が再び減る現象。"
    },
    {
      "term": "ノーフリーランチの定理",
      "meaning": "すべての問題に最適な学習アルゴリズムは存在しないという定理。"
    },
    {
      "term": "ハイパーパラメータ",
      "meaning": "学習率などモデル外側で設定するパラメータ。"
    },
    {
      "term": "バッチ学習",
      "meaning": "全データを用いて更新する学習方法。"
    },
    {
      "term": "ミニバッチ学習",
      "meaning": "データを小さな塊に分けて学習する方法。"
    },
    {
      "term": "モーメンタム",
      "meaning": "過去の勾配を利用して更新を安定化する手法。"
    },
    {
      "term": "ランダムサーチ",
      "meaning": "ランダムにハイパーパラメータを探索する方法。"
    }
  ]
}