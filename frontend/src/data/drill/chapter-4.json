{
  "slug": "chapter-4",
  "title": "第4章 ディープラーニングの概要 - ドリル",
  "description": "AIの定義と代表例を復習する基本問題",
  "questions": [
    {
      "prompt": "人工知能（AI）を最も適切に表す説明はどれか？",
      "choices": [
        "大量データを保存するためのデータベース技術",
        "仮想現実の映像生成に特化したアルゴリズム群",
        "人間の知的行為を計算機で実現しようとする技術の総称",
        "ネットワーク通信を最適化するためのプロトコル"
      ],
      "answerIndex": 3,
      "explanation": "人工知能（AI）は、人間が行う認知・判断・推論・学習といった知的行為をコンピュータで模倣しようとする技術全般を指す。データベースや通信技術とは目的が異なる。"
    },
    {
      "prompt": "弱いAI（Narrow AI）の例として最も近いものはどれか？",
      "choices": [
        "将棋やチェスに特化したプログラム",
        "人間と同等の一般知能を持つシステム",
        "複数の分野で自律的に目的を設定する汎用 AI",
        "自らの意識を持って判断する AI"
      ],
      "answerIndex": 0,
      "explanation": "弱いAI（特化型AI）は、特定のタスクだけを高性能で実行するAIを指す。将棋AIや画像分類AIなどが該当し、汎用的な知能や意識は持たない。"
    },
    {
      "prompt": "チューリングテストの目的はどれか？",
      "choices": [
        "コンピュータの計算速度を測定する",
        "AIの知的応答が人間と区別できるかを評価する",
        "プログラムのセキュリティ脆弱性を検査する",
        "ネットワークの通信遅延を計測する"
      ],
      "answerIndex": 1,
      "explanation": "チューリングテストは、AI の応答が人間と見分けがつかないかどうかを確認するためのテスト。知能そのものを直接測定するものではない。"
    },
    {
      "prompt": "機械学習（Machine Learning）の説明として最も適切なものはどれか？",
      "choices": [
        "データから規則やパターンを学びモデルを構築する手法",
        "画像を高解像度化するための単独アルゴリズム",
        "ソフトウェアのテスト工程を自動化するツール群",
        "暗号化通信を実現するためのプロトコル"
      ],
      "answerIndex": 0,
      "explanation": "機械学習は大量のデータからパターンを学習し、予測・分類などのモデルを構築する手法を指す。画像処理や通信など特定用途に限定されない。"
    },
    {
      "prompt": "シンギュラリティ（技術的特異点）とは何を指すか？",
      "choices": [
        "AI が人間の知能を大きく超え、社会が急変するとされる転換点",
        "ニューラルネットの重みが収束する閾値",
        "データセンターの容量が限界に達するポイント",
        "アルゴリズムの計算量が最小化される状態"
      ],
      "answerIndex": 0,
      "explanation": "シンギュラリティは、AI が人間の知能を超え、社会構造が急速に変化するとされる未来の転換点を意味する概念。レイ・カーツワイルなどが提唱した。"
    },
    {
      "prompt": "ニューラルネットワークに関する記述として、最も不適切なものを選べ",
      "choices": [
        "ニューラルネットワークは、脳の神経回路網に着想を得て考案された機械学習モデルである",
        "ニューラルネットワークは、複数のニューロンの集合から構成される層をもつ",
        "ニューラルネットワークの層の数を非常に大きくしたネットワークは、ディープニューラルネットワークと呼ばれる",
        "ニューラルネットワークは、分類タスクのみを解くことができる機械学習モデルである"
      ],
      "answerIndex": 3,
      "explanation": "ニューラルネットワークは、回帰や分類など、様々なタスクを解くことができる。"
    },
    {
      "prompt": "ディープラーニングでは、学習によって最適な特徴量の抽出方法が獲得される。このことを表す用語として、最も適切なものを選べ。",
      "choices": [
        "能動学習",
        "特徴表現学習",
        "教師なし学習",
        "強化学習"
      ],
      "answerIndex": 1,
      "explanation": "ディープラーニングでは、特徴量の抽出過程そのものを学習することが可能である。"
    },
    {
      "prompt": "ディープラーニングに関する記述として、最も不適切なものを選べ。",
      "choices": [
        "ディープラーニングは、ニューラルネットワークの層の数を非常に大きくしたディープニューラルネットワークを用いる機械学習手法である",
        "ディープラーニングでは、ネットワークの複雑さに対して学習データが少ない場合、過学習が発生しやすい",
        "ディープラーニングは、画像認識や自然言語処理など多くの分野で高い性能を実現し、各分野で技術的ブレークスルーをもたらしている",
        "ディープラーニングは、予測の根拠を説明できるためホワイトボックスモデルと呼ばれている"
      ],
      "answerIndex": 3,
      "explanation": "ディープラーニングでは"
    },
    {
      "prompt": "1958年に発表された単純パーセプトロンに関する説明として、最も不適切なものを選べ。",
      "choices": [
        "単純パーセプトロンは、ニューラルネットワークの元祖と呼ばれることがある",
        "単純パーセプトロンは、線形分離不可能な問題を解くことができない",
        "単純パーセプトロンは、2層以上の隠れ層を備えている",
        "単純パーセプトロンは、2クラス分類を行うことができる"
      ],
      "answerIndex": 2,
      "explanation": ""
    },
    {
      "prompt": "ディープラーニングの学習に必要なデータに関する説明として、最も適切なものを選べ。",
      "choices": [
        "ディープラーニングにおける学習データで大切なのはデータの質であり、データの量は関係ない",
        "ディープラーニングの学習では、必ずネットワークのパラメー夕数と同等のデータ量を確保する必要がある",
        "ディープラーニングの学習には大量のデータが必要となるが、必要なデータ量に関する共通の基準は存在しない",
        "データ量が少なくても、ほかの手法を用いずにディープラーニングを用いる方がよい"
      ],
      "answerIndex": 2,
      "explanation": ""
    },
    {
      "prompt": "コンピュータの演算処理装置のひとつとしてGPU （Graphics Processing Unit）がある。GPUに関する説明として、最も適切なものを選べ。",
      "choices": [
        "GPUは、コンピュータ全般の作業を処理する役割を担う",
        "GPUは、大規模な並列演算処理に特化している",
        "GPUは、画像を扱うのが得意な一方、動画を扱うのは得意ではない",
        "GPUは、処理方法の異なる多様なタスクを問時に処理することが得意である"
      ],
      "answerIndex": 1,
      "explanation": ""
    },
    {
      "prompt": "ニューラルネットワ一クにおける活性化開数に関する説明として、最も適切なものを選べ。",
      "choices": [
        "シグモイド関数は、ReLU （Rectified Linear Unit）と比較して勾配消失問題が発生しやすい",
        "ReLU （Rectified Linear Unit）は、入力が0以上の領域では必ず0を出力する関数である",
        "Leaky ReLU （Leaky Rectified Linear Unit）は、入力が0以上の領域で非線形な関数である",
        "tanh関数は、ReLU （Rectified Linear Unit）と比較して勾配消失問題が発生しにくい"
      ],
      "answerIndex": 0,
      "explanation": ""
    },
    {
      "prompt": "ニューラルネットワークを使用して線形分離不可能な問題を解く際に、隠れ層の活性化関数に必ず求められる条件として、最も適切なものを選べ。",
      "choices": [
        "線形であること",
        "出力値が0から1の範囲であること",
        "受け取れる入力値の範囲が0以上であること",
        "非線形であること"
      ],
      "answerIndex": 3,
      "explanation": ""
    },
    {
      "prompt": "活性化関数のひとつであるLeaky ReLU （Leaky Rectified Linear Unit）に関する説明として、最も適切なものを選べ。",
      "choices": [
        "Leaky ReLUは、0以上の値の入力に対しては入力と同じ値を出力し、0未満の値の入力に対しては・1を出力する",
        "Leaky ReLUは、勾配消失がまったく発生しない関数である",
        "Leaky ReLUは、すべての範囲で一定の傾きをもつ線形関数である",
        "Leaky ReLUは、入力が負の領域でもわずかな傾きをもっており、ReLUに比べて勾配消失が起きにくい"
      ],
      "answerIndex": 3,
      "explanation": ""
    },
    {
      "prompt": "分類タスクを解くためのニューラルネットワークの出力層に適用する活性化関数として、最も適切なものを選べ。",
      "choices": [
        "恒等写像関数",
        "ReLU (Rectified Linear Unit)",
        "ソフトマックス関数",
        "Leaky ReLU (Leaky Rectified Linear Unit)"
      ],
      "answerIndex": 2,
      "explanation": ""
    },
    {
      "prompt": "機械学習モデルの予測値と教師データとの誤差を計算するための関数として、最も適切なものを選べ。",
      "choices": [
        "損失関数",
        "差分関数",
        "残差関数",
        "分散関数"
      ],
      "answerIndex": 0,
      "explanation": ""
    },
    {
      "prompt": "機械学習における正則にの主要な目的として、最も適切なものを選べ。",
      "choices": [
        "正則化の目的は、データの外れ値の影響を小さくすることである",
        "正則化の目的は、モデルの学習速度を上げることである",
        "正則化の目的は、モデルに入力する特徴量を作り出すことである",
        "正則化の目的は、過学習を防いで汎化性能を向上させることである"
      ],
      "answerIndex": 3,
      "explanation": ""
    },
    {
      "prompt": "L1正則化に関する説明として、最も適切なものを選べ。",
      "choices": [
        "L1正則化は、パラメータの大きさの2乗和を損失関数に加えることで、正則化を行う手法である",
        "L1正則化は、パラメータの大きさの絶対値の総和を損失関数に加えることで、正則化を行う手法である",
        "L1正則化は、0でない大きさをもつパラメータの総数を損失関数に加えることで、正則化を行う手法である",
        "L1正則化は、パラメータの大きさをすべてかけ合わせた値を類失関数に加えることで、正則化を行う手法である"
      ],
      "answerIndex": 1,
      "explanation": ""
    },
    {
      "prompt": "ニューラルネットワークの学習に用いられるドロップアウトに関する説明として、最も不適切なものを選べ。",
      "choices": [
        "ドロップアウトは、学習時に訓練データをランダムに除外する手法である",
        "ドロップアウトは、学習時にニューロンをランダムに除外する手法である",
        "ドロップアウトは、過学習を抑制するテクニックである",
        "ドロップアウトによる学習は、アンサンブル学習とみなすことができる"
      ],
      "answerIndex": 0,
      "explanation": ""
    },
    {
      "prompt": "ある点は、ある次元では極小であるが、別の次元では極大となる。このような点に陥ると、学習が進みにくくなることがある。この点として最も適切なものを選べ。",
      "choices": [
        "局所最適解",
        "大域最適解",
        "鞍点",
        "原点"
      ],
      "answerIndex": 2,
      "explanation": ""
    },
    {
      "prompt": "ある手法は、勾配降下法の改良法のひとつであり、過去の勾配の情報を利用して効率よく最適化を進める。この手法を用いると、鞍点などで学習が停滞することを防げる場合がある。この手法として最も適切なものを選べ。",
      "choices": [
        "ドロップアウト",
        "確率的勾配降下法",
        "モーメンタム",
        "早期終了"
      ],
      "answerIndex": 2,
      "explanation": ""
    },
    {
      "prompt": "勾配降下法にはさまざまな手法が存在する。それらのうち、AdaGrad以降に提案された手法として、最も不適切なものを選べ。",
      "choices": [
        "NAG",
        "RMSprop",
        "Adam",
        "AMSBound"
      ],
      "answerIndex": 0,
      "explanation": ""
    },
    {
      "prompt": "エポックは、ニューラルネットワークの学習を行う際に使用される懸念である。エポックに関する説明として、最も適切なものを選べ。",
      "choices": [
        "学習の繰り返し計算において、訓練データを一周すると1エポックと数える",
        "学習の繰り返し計算において、訓練データを十周すると1エポックと数える",
        "学習の繰り返し計算において、訓練データを百周すると1エポックと数える",
        "学習の繰り返し計算において、訓練データを千周すると1エポックと数える"
      ],
      "answerIndex": 0,
      "explanation": ""
    },
    {
      "prompt": "27ニューラルネットワークの学習において、二重降下現象と呼ばれる現象が発生することがある。二重降下現象に関する説明として最も適切なものを選べ。",
      "choices": [
        "二重降下現象とは、学習中に訓練データにおける誤差が減少し、テストデータにおける誤差が増加する現象である",
        "二重降下現象とは、学習中に訓練データとテストデータにおける誤差が共に減少する現象である",
        "二重降下現象とは、ニューラルネットワークの層が増えることで過学習が進む現象である",
        "二重降下現象とは、学習中に減少していたテストデータに対する誤差が一度増加したあと、再び減少する現象である"
      ],
      "answerIndex": 3,
      "explanation": ""
    },
    {
      "prompt": "ノーフリーランチ定理は、1995年に証明され、今日の機械学習においてもよく参照される定理である。ノーフリーランチ定理に関する説明として、最も適切なものを選べ。",
      "choices": [
        "ノーフリーランチ定理とは、単純なモデルが複雑なモデルよりも過学習しにくいことを示す定理である",
        "ノーフリーランチ定理とは、複雑なモデルによる推論の根拠を示すことが難しいことを示す定理である",
        "ノーフリーランチ定理とは、あらゆる問題において優れた汎化性能をもつモデルは存在しないことを示す定理である",
        "ノーフリーランチ定理とは、モデルの誤差をバイアスとバリアンスに分解することができるという定理である"
      ],
      "answerIndex": 2,
      "explanation": ""
    },
    {
      "prompt": "機械学習では、ハイパーパラメータと呼ばれる概念がある。ハイパーパラメータに関する説明として、最も適切なものを選べ。",
      "choices": [
        "ハイパーパラメータは、機械学習アルゴリズムそのものによって調製するものである",
        "ハイパーパラメータは、モデルの学習前に設定するものである",
        "ハイパーパラメータは、モデルが学習可能なパラメータのうち特に重視すべきものである",
        "ハイパーパラメータは、モデルの精度に影響を与えないものである"
      ],
      "answerIndex": 1,
      "explanation": ""
    },
    {
      "prompt": "ハイパーパラメータを探索する手法のひとつにグリッドサーチがある。グリッドサーチに関する説明として、最も適切なものを選べ。",
      "choices": [
        "グリッドサーチは、ハイパーパラメータの候補からランダムに選択しながら探索する手法である",
        "グリッドサーチは、ハイパーパラメータの候補のすべての組み合わせを探索する手法である",
        "グリッドサーチは、探索の過程において、その時点までの探索結果を踏まえて次の探索点を決定する手法である",
        "グリッドサーチは、突然変異や世代交代といった概念を用いて探索を行う手法である"
      ],
      "answerIndex": 1,
      "explanation": ""
    }
  ]
}
