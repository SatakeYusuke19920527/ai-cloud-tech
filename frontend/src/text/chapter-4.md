# 第 4 章：ディープラーニングの概要

---

## 4.1 ディープラーニングとは

ディープラーニング（Deep Learning）とは、多層のニューラルネットワークを用いて  
データから高度な特徴を自動的に学習する手法である。

- 人間の脳の神経回路を模したニューラルネットワークがベース
- 特徴量エンジニアリングを自動化
- 画像・音声・自然言語などの非構造データに強い
- GPU/TPU などの計算資源の発展によって急速に進化

---

## 4.2 ディープラーニングが注目された背景

### ■ 背景 1：大量データ（Big Data）の時代

- Web、SNS、IoT により膨大なデータが生成されるようになった
- ディープラーニングは大量のデータで性能が向上する

### ■ 背景 2：計算パワーの向上

- GPU/TPU による並列計算で学習が高速化
- クラウドサービスにより安価に利用可能になった

### ■ 背景 3：アルゴリズムの進化

- 新たなモデル（CNN、RNN、LSTM、Transformer など）の登場
- 正則化手法、最適化手法などの改善

---

## 4.3 ニューラルネットワークの基本構造

### ■ ニューラルネットワークの層構造

1. **入力層（Input Layer）**
2. **中間層（Hidden Layers）**：深いほど表現力が向上
3. **出力層（Output Layer）**

### ■ パラメータ

- **重み（Weights）**
- **バイアス（Bias）**
- **活性化関数（Activation Function）**：ReLU, Sigmoid, Tanh など

---

## 4.4 ディープラーニングの代表的なモデル

### ● CNN（畳み込みニューラルネットワーク）

- 画像認識で圧倒的に強い
- 畳み込み層により局所特徴を抽出
- AlexNet、VGG、ResNet など

### ● RNN（リカレントニューラルネットワーク）

- 時系列データ・自然言語に強い
- LSTM、GRU により長期依存問題を改善

### ● Transformer（トランスフォーマー）

- 現代 AI の主流
- Attention 機構により長距離依存を効率的に学習
- BERT, GPT シリーズ, Vision Transformer など

---

## 4.5 ディープラーニングによる性能向上の理由

- 多層構造により抽象度の高い特徴を自動学習できる
- 非線形の表現能力が高い
- End-to-End で最適化できる
- モデル規模を大きくするとさらに精度向上が見込める

---

## 4.6 ディープラーニングの課題

### ■ 課題 1：大量データと計算コスト

- 学習に膨大なデータと GPU が必要

### ■ 課題 2：ブラックボックス性

- 何を学習しているか解釈が難しい

### ■ 課題 3：過学習のリスク

- 正則化やドロップアウトが必要

### ■ 課題 4：倫理問題

- バイアス・公平性
- 説明責任・安全性

---

## 4.7 まとめ

- ディープラーニングは AI 発展の中心となる技術
- 非構造データに強く、さまざまな分野で性能向上を実現
- 課題もあるため技術と倫理の両面が重要
